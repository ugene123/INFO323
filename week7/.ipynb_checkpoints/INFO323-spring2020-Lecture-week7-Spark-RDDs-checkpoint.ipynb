{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center\"> INFO 323: Cloud Computing and Big Data</h1>\n",
    "<h2 style=\"text-align:center\"> College of Computing and Informatics</h2>\n",
    "<h2 style=\"text-align:center\">Drexel University</h2>\n",
    "\n",
    "<h3 style=\"text-align:center\"> Week 7: Resilient Distributed Datasets (Ch 12: RDDs)</h3>\n",
    "<h3 style=\"text-align:center\"> Yuan An, PhD</h3>\n",
    "<h3 style=\"text-align:center\">Associate Professor</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  About RDDs\n",
    "Virtually all Spark code you run,\n",
    "whether DataFrames or Datasets, compiles down to an RDD. \n",
    "In short, an RDD represents an immutable, partitioned collection of records that can be operated on in\n",
    "parallel. Unlike DataFrames though, where each record is a structured row containing fields with a\n",
    "known schema, in RDDs the records are just Java, Scala, or Python objects of the programmer’s\n",
    "choosing.\n",
    "\n",
    "RDDs give you complete control because every record in an RDD is a just a Java or Python object.\n",
    "You can store anything you want in these objects, in any format you want. This gives you great power,\n",
    "but not without potential issues. Every manipulation and interaction between values must be defined\n",
    "by hand, meaning that you must “reinvent the wheel” for whatever task you are trying to carry out.\n",
    "\n",
    "Also, optimizations are going to require much more manual work, because Spark does not understand\n",
    "the inner structure of your records as it does with the Structured APIs. For instance, Spark’s\n",
    "Structured APIs automatically store data in an optimzied, compressed binary format, so to achieve the\n",
    "same space-efficiency and performance,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating RDDs: Interoperating Between DataFrames and RDDs\n",
    "One of the easiest ways to get RDDs is from an existing DataFrame or Dataset. Converting these to an\n",
    "RDD is simple: just use the rdd method on any of these data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rd = spark.range(10).rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rd.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To operate on this data, you will need to convert this Row object to the correct data type or extract\n",
    "values out of it, as shown in the example that follows. This is now an RDD of type Row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "rd = spark.range(10).toDF(\"id\").rdd.map(lambda row: row[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rd.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the same methodology to create a DataFrame or Dataset from an RDD. All you need to\n",
    "do is call the toDF method on the RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "df = spark.range(10).rdd.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From a Local Collection\n",
    "To create an RDD from a collection, you will need to use the parallelize method on a\n",
    "SparkContext (within a SparkSession). This turns a single node collection into a parallel collection.\n",
    "When creating this parallel collection, you can also explicitly state the number of partitions into\n",
    "which you would like to distribute this array. In this case, we are creating two partitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "myCollection = \"Spark The Definitive Guide : Big Data Processing Made Simple\"\\\n",
    "  .split(\" \")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = spark.sparkContext.parallelize(myCollection, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An additional feature is that you can then name this RDD to show up in the Spark UI according to a\n",
    "given name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "words.setName(\"myWords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words.name() # myWords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## filter\n",
    "Filtering is equivalent to creating a SQL-like where clause. You can look through our records in the\n",
    "RDD and see which ones match some predicate function. This function just needs to return a Boolean\n",
    "type to be used as a filter function. The input should be whatever your given row is. In this next\n",
    "example, we filter the RDD to keep only the words that begin with the letter “S”:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "def startsWithS(individual):\n",
    "  return individual.startswith(\"S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we defined the function, let’s filter the data. This should feel quite familiar if you read\n",
    "Chapter 11 because we simply use a function that operates record by record in the RDD. The function\n",
    "is defined to work on each record in the RDD individually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "words.filter(lambda word: startsWithS(word)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## map\n",
    "Mapping is again the same operation that you can read. You specify a function\n",
    "that returns the value that you want, given the correct input. You then apply that, record by record.\n",
    "Let’s perform something similar to what we just did. In this example, we’ll map the current word to\n",
    "the word, its starting letter, and whether the word begins with “S.”\n",
    "Notice in this instance that we define our functions completely inline using the relevant lambda\n",
    "syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "words2 = words.map(lambda word: (word, word[0], word.startswith(\"S\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can subsequently filter on this by selecting the relevant Boolean value in a new function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "words2.filter(lambda record: record[2]).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## flatMap\n",
    "flatMap provides a simple extension of the map function we just looked at. Sometimes, each current\n",
    "row should return multiple rows, instead. For example, you might want to take your set of words and\n",
    "flatMap it into a set of characters. Because each word has multiple characters, you should use\n",
    "flatMap to expand it. flatMap requires that the ouput of the map function be an iterable that can be\n",
    "expanded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "words.flatMap(lambda word: list(word)).take(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sort\n",
    "To sort an RDD you must use the sortBy method, and just like any other RDD operation, you do this\n",
    "by specifying a function to extract a value from the objects in your RDDs and then sort based on that.\n",
    "For instance, the following example sorts by word length from longest to shortest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "words.sortBy(lambda word: len(word) * -1).take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Splits\n",
    "We can also randomly split an RDD into an Array of RDDs by using the randomSplit method,\n",
    "which accepts an Array of weights and a random seed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "fiftyFiftySplit = words.randomSplit([0.5, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiftyFiftySplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reduce\n",
    "You can use the reduce method to specify a function to “reduce” an RDD of any kind of value to one\n",
    "value. For instance, given a set of numbers, you can reduce this to its sum by specifying a function that\n",
    "takes as input two values and reduces them into one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "spark.sparkContext.parallelize(range(1, 21)).reduce(lambda x, y: x + y) # 210"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use this to get something like the longest word in our set of words that we defined a\n",
    "moment ago. The key is just to define the correct function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "def wordLengthReducer(leftWord, rightWord):\n",
    "  if len(leftWord) > len(rightWord):\n",
    "    return leftWord\n",
    "  else:\n",
    "    return rightWord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words.reduce(wordLengthReducer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
