{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center\"> INFO 323: Cloud Computing and Big Data</h1>\n",
    "<h2 style=\"text-align:center\"> College of Computing and Informatics</h2>\n",
    "<h2 style=\"text-align:center\">Drexel University</h2>\n",
    "\n",
    "<h3 style=\"text-align:center\">Lecture for Week 7</h3>\n",
    "<h3 style=\"text-align:center\"> Spark's Advanced Analytics Tools (Definitive 24)</h3>\n",
    "<h3 style=\"text-align:center\"> Yuan An, PhD</h3>\n",
    "<h3 style=\"text-align:center\">Associate Professor</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark's Advanced Analytics Tools\n",
    "Spark includes several core packages and many\n",
    "external packages for performing advanced analytics. The primary package is MLlib, which provides\n",
    "an interface for building machine learning pipelines.\n",
    "\n",
    "### What Is MLlib?\n",
    "MLlib is a package, built on and included in Spark, that provides interfaces for gathering and cleaning\n",
    "data, feature engineering and feature selection, training and tuning large-scale supervised and\n",
    "unsupervised machine learning models, and using those models in production.\n",
    "\n",
    "### When and why should you use MLlib (versus scikit-learn, TensorFlow, etc)\n",
    "At a high level, MLlib might sound like a lot of other machine learning packages you’ve probably\n",
    "heard of, such as scikit-learn for Python or the variety of R packages for performing similar tasks. \n",
    "\n",
    "So why should you bother with MLlib at all? There are numerous tools for performing machine learning\n",
    "on a single machine, and while there are several great options to choose from, these single machine\n",
    "tools do have their limits either in terms of the size of data you can train on or the processing time.\n",
    "\n",
    "This means single-machine tools are usually complementary to MLlib. When you hit those scalability\n",
    "issues, take advantage of Spark’s abilities.\n",
    "\n",
    "There are two key use cases where you want to leverage Spark’s ability to scale. \n",
    "1. ** First, you want to leverage Spark for preprocessing and feature generation to reduce the amount of time it might take to produce training and test sets from a large amount of data. Then you might leverage single-machine\n",
    "learning libraries to train on those given data sets. **\n",
    "\n",
    "2. ** Second, when your input data or model size become too difficult or inconvenient to put on one machine, use Spark to do the heavy lifting. Spark makes distributed machine learning very simple. **\n",
    "\n",
    "An important caveat to all of this is that while training and data preparation are made simple, there\n",
    "are still some complexities you will need to keep in mind, especially when it comes to deploying a\n",
    "trained model. For example, ** Spark does not provide a built-in way to serve low-latency predictions\n",
    "from a model, so you may want to export the model to another serving system or a custom application\n",
    "to do that. **\n",
    "\n",
    "MLlib is generally designed to allow inspecting and exporting models to other tools where\n",
    "possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Types for Spark ML\n",
    "There are also several lower-level data types\n",
    "you may need to work with in MLlib (Vector being the most common). Whenever we pass a set of\n",
    "features into a machine learning model, we must do it as a vector that consists of Doubles. \n",
    "\n",
    "This vector can be either sparse (where most of the elements are zero) or dense (where there are many\n",
    "unique values). Vectors are created in different ways. To create a dense vector, we can specify an\n",
    "array of all the values. To create a sparse vector, we can specify the total size and the indices and\n",
    "values of the non-zero elements. Sparse is the best format, as you might have guessed, when the\n",
    "majority of values are zero as this is a more compressed representation. Here is an example of how to\n",
    "manually create a Vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector in Spark MLLib\n",
    "from pyspark.ml.linalg import Vectors\n",
    "denseVec = Vectors.dense(1.0, 2.0, 3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denseVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 3\n",
    "idx = [1, 2] # locations of non-zero elements in vector\n",
    "values = [2.0, 3.0]\n",
    "sparseVec = Vectors.sparse(size, idx, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denseVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparseVec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLlib in Action\n",
    "We’ll use a small synthetic dataset that will\n",
    "help illustrate Spark's ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(\"gs://info323-ya45-spring2020-bucket/data/simple-ml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.orderBy(\"value2\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset consists of a categorical label with two values (good or bad), a categorical variable\n",
    "(color), and two numerical variables. Suppose that we want to train a classification model where we\n",
    "hope to predict a binary variable—the label—from the other values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering with Transformers\n",
    "When we use MLlib, all inputs to machine learning algorithms in Spark must consist of type Double (for labels) and Vector[Double] (for features).\n",
    "\n",
    "The current dataset does not meet that requirement and therefore we need to transform it to the proper\n",
    "format.\n",
    "\n",
    "To achieve this in our example, we are going to specify an RFormula. This is a declarative language\n",
    "for specifying machine learning transformations and is simple to use once you understand the syntax.\n",
    "\n",
    "RFormula supports a limited subset of the R operators that in practice work quite well for simple\n",
    "models and manipulations. The\n",
    "basic RFormula operators are:\n",
    "\n",
    "~\n",
    "\n",
    "Separate target and terms\n",
    "\n",
    "+\n",
    "\n",
    "Concat terms; “+ 0” means removing the intercept (this means that the y-intercept of the line that\n",
    "we will fit will be 0)\n",
    "\n",
    "-\n",
    "\n",
    "Remove a term; “- 1” means removing the intercept (this means that the y-intercept of the line that\n",
    "we will fit will be 0—yes, this does the same thing as “+ 0”\n",
    "\n",
    ":\n",
    "\n",
    "Interaction (multiplication for numeric values, or binarized categorical values)\n",
    "\n",
    ".\n",
    "\n",
    "All columns except the target/dependent variable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to specify transformations with this syntax, we need to import the relevant class. Then we go\n",
    "through the process of defining our formula. In this case we want to use all available variables (the .)\n",
    "\n",
    "Later, we can try to add in the interactions between value1 and color and value2 and color, treating those as\n",
    "new features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('color').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('value1').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RFormula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supervised = RFormula(formula=\"lab ~ color:value1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fittedRF = supervised.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preparedDF = fittedRF.transform(df)\n",
    "preparedDF.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supervised = RFormula(formula=\"lab~.+color:value1+color:value2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have declaratively specified how we would like to change our data into what we\n",
    "will train our model on. The next step is to fit the RFormula transformer to the data to let it discover\n",
    "the possible values of each column. Not all transformers have this requirement but because RFormula\n",
    "will automatically handle categorical variables for us, it needs to determine which columns are\n",
    "categorical and which are not, as well as what the distinct values of the categorical columns are. For\n",
    "this reason, we have to call the fit method. Once we call fit, it returns a “trained” version of our\n",
    "transformer we can then use to actually transform our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fittedRF = supervised.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preparedDF = fittedRF.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preparedDF.show(7, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It assigns a numerical value to each possible color category, creates additional\n",
    "features for the interaction variables between colors and value1/value2, and puts them all into a\n",
    "single vector. We then call transform on that object in order to transform our input data into the\n",
    "expected output data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s create a simple test set based off a random split of the data now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preparedDF.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preparedDF.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = preparedDF.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "Now that we have transformed our data into the correct format and created some valuable features,\n",
    "it’s time to actually fit our model. To create our classifier we instantiate an instance of LogisticRegression, using the\n",
    "default configuration or hyperparameters. We then set the label columns and the feature columns; the\n",
    "column names we are setting—label and features—are actually the default labels for all\n",
    "estimators in Spark MLlib, and in later chapters we omit them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(labelCol=\"label\",featuresCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we actually go about training this model, let’s inspect the parameters. This is also a great way\n",
    "to remind yourself of the options available for each particular model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lr.explainParams())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It shows an explanation of all of the parameters for\n",
    "Spark’s implementation of logistic regression. The explainParams method exists on all algorithms\n",
    "available in MLlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fittedLR = lr.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once complete, you can use the model to make predictions. Logically this means tranforming features\n",
    "into labels. We make predictions with the transform method. For example, we can transform our\n",
    "training dataset to see what labels our model assigned to the training data and how those compare to\n",
    "the true outputs. This, again, is just another DataFrame we can manipulate. Let’s perform that\n",
    "prediction with the following code snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fittedLR.transform(train).select(\"label\", \"prediction\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next step would be to manually evaluate this model and calculate performance metrics like the\n",
    "true positive rate, false negative rate, and so on. We might then turn around and try a different set of\n",
    "parameters to see if those perform better. However, while this is a useful process, it can also be quite\n",
    "tedious. Spark helps you avoid manually trying different models and evaluation criteria by allowing\n",
    "you to specify your workload as a declarative pipeline of work that includes all your transformations\n",
    "as well as tuning your hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelining Our Workflow\n",
    "As you probably noticed, if you are performing a lot of transformations, writing all the steps and\n",
    "keeping track of DataFrames ends up being quite tedious. That’s why Spark includes the Pipeline\n",
    "concept. A pipeline allows you to set up a dataflow of the relevant transformations that ends with an\n",
    "estimator that is automatically tuned according to your specifications, resulting in a tuned model ready\n",
    "for use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create held out set\n",
    "train, test = df.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two transformers\n",
    "rForm = RFormula()\n",
    "lr = LogisticRegression().setLabelCol(\"label\").setFeaturesCol(\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now instead of manually using\n",
    "our transformations and then tuning our model we just make them stages in the overall pipeline, as in\n",
    "the following code snippet:    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "stages = [rForm, lr]\n",
    "pipeline = Pipeline().setStages(stages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Evaluation\n",
    "Now that you arranged the logical pipeline, the next step is training. In our case, we won’t train just\n",
    "one model (like we did previously); we will train several variations of the model by specifying\n",
    "different combinations of hyperparameters that we would like Spark to test. We will then select the\n",
    "best model using an Evaluator that compares their predictions on our validation data. We can test\n",
    "different hyperparameters in the entire pipeline, even in the RFormula that we use to manipulate the\n",
    "raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "params = ParamGridBuilder()\\\n",
    ".addGrid(rForm.formula, [\n",
    "\"lab ~ . + color:value1\",\n",
    "\"lab ~ . + color:value1 + color:value2\"])\\\n",
    ".addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\\\n",
    ".addGrid(lr.regParam, [0.1, 2.0])\\\n",
    ".build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build 12 different logistic regression models. The evaluator allows us to\n",
    "automatically and objectively compare multiple models to the same evaluation metric.  We will use the\n",
    "BinaryClassificationEvaluator, which has a number of potential evaluation metrics. In this case we will use areaUnderROC, which is the total area under the\n",
    "receiver operating characteristic, a common measure of classification performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator()\\\n",
    ".setMetricName(\"areaUnderROC\")\\\n",
    ".setRawPredictionCol(\"prediction\")\\\n",
    ".setLabelCol(\"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark provides two options for performing\n",
    "hyperparameter tuning automatically. We can use TrainValidationSplit, which will simply\n",
    "perform an arbitrary random split of our data into two different groups, or CrossValidator, which\n",
    "performs K-fold cross-validation by splitting the dataset into k non-overlapping, randomly partitioned\n",
    "folds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import TrainValidationSplit\n",
    "tvs = TrainValidationSplit()\\\n",
    ".setTrainRatio(0.75)\\\n",
    ".setEstimatorParamMaps(params)\\\n",
    ".setEstimator(pipeline)\\\n",
    ".setEvaluator(evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and evaluate the models. tvs stands for trainValidationSplit\n",
    "tvsFitted = tvs.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.evaluate(tvsFitted.transform(test)) // 0.9166666666666667"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the single model trained early"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = preparedDF.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.evaluate(fittedLR.transform(test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
