{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFO 323: Cloud Computing and Big Data\n",
    "## An End-to-End Example for Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sark reads in a DataFrame from a file. The DataFrame has a set of columns with an unspecified number of rows. The reason the number of rows is unspecified is because reading data is a transformation, and\n",
    "is therefore a lazy operation. Spark peeked at only a couple of rows of data to try to guess what types\n",
    "each column should be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-04c91a246ae7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mflightdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moption\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"inferSchema\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"true\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moption\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"header\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"true\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"2015-summary.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "flightdata = spark.read.option(\"inferSchema\", \"true\").option(\"header\", \"true\").csv(\"2015-summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland', count=344)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flightdata.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us specify a wide transformation, sort(). Nothing happens to the data when we call sort because it’s just a transformation. However, we can\n",
    "see that Spark is building up a plan for how it will execute this across the cluster by looking at the\n",
    "explain plan. We can call explain on any DataFrame object to see the DataFrame’s lineage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) Sort [count#12 ASC NULLS FIRST], true, 0\n",
      "+- Exchange rangepartitioning(count#12 ASC NULLS FIRST, 200)\n",
      "   +- *(1) FileScan csv [DEST_COUNTRY_NAME#10,ORIGIN_COUNTRY_NAME#11,count#12] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/home/cloudera/spark-definitive/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int>\n"
     ]
    }
   ],
   "source": [
    "flightdata.sort(\"count\").explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a sequence of transofrmations as: narror (read) -> wide (sort)\n",
    "\n",
    "You can read explain plans from top to bottom, the top being the\n",
    "end result, and the bottom being the source(s) of data. In this case, take a look at the first keywords.\n",
    "You will see sort, exchange, and FileScan. That’s because the sort of our data is actually a wide\n",
    "transformation because rows will need to be compared with one another. Don’t worry too much about\n",
    "understanding everything about explain plans at this point, they can just be helpful tools for debugging\n",
    "and improving your knowledge as you progress with Spark.\n",
    "\n",
    "Next, we can specify an action to kick off this plan. However, before doing\n",
    "that, we’re going to set a configuration. By default, when we perform a shuffle, Spark outputs 200\n",
    "shuffle partitions. Let’s set this value to 5 to reduce the number of the output partitions from the\n",
    "shuffle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Singapore', count=1),\n",
       " Row(DEST_COUNTRY_NAME='Moldova', ORIGIN_COUNTRY_NAME='United States', count=1)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flightdata.sort(\"count\").take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame and SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We worked through a simple transformation in the previous example, let’s now work through a more\n",
    "complex one and follow along in both DataFrames and SQL. Spark can run the same transformations,\n",
    "regardless of the language, in the exact same way. You can express your business logic in SQL or\n",
    "DataFrames (either in R, Python, Scala, or Java) and Spark will compile that logic down to an\n",
    "underlying plan (that you can see in the explain plan) before actually executing your code. With Spark\n",
    "SQL, you can register any DataFrame as a table or view (a temporary table) and query it using pure\n",
    "SQL. There is no performance difference between writing SQL queries or writing DataFrame code,\n",
    "they both “compile” to the same underlying plan that we specify in DataFrame code.\n",
    "You can make any DataFrame into a table or view with one simple method call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightdata.createOrReplaceTempView(\"flight_data_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can query our data in SQL. To do so, we’ll use the spark.sql function (remember, spark is our SparkSession variable) that conveniently returns a new DataFrame. Although this might seem a bit circular in logic—that a SQL query against a DataFrame returns another DataFrame—it’s actually\n",
    "quite powerful. This makes it possible for you to specify transformations in the manner most convenient to you at any given point in time and not sacrifice any efficiency to do so! To understand\n",
    "that this is happening, let’s take a look at two explain plans:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[DEST_COUNTRY_NAME#10], functions=[count(1)])\n",
      "+- Exchange hashpartitioning(DEST_COUNTRY_NAME#10, 5)\n",
      "   +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME#10], functions=[partial_count(1)])\n",
      "      +- *(1) FileScan csv [DEST_COUNTRY_NAME#10] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/home/cloudera/spark-definitive/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>\n"
     ]
    }
   ],
   "source": [
    "flightdata.groupBy(\"DEST_COUNTRY_NAME\").count().explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlWay = spark.sql(\"\"\"\n",
    "SELECT DEST_COUNTRY_NAME, count(1)\n",
    "FROM flight_data_table\n",
    "GROUP BY DEST_COUNTRY_NAME\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[DEST_COUNTRY_NAME#10], functions=[count(1)])\n",
      "+- Exchange hashpartitioning(DEST_COUNTRY_NAME#10, 5)\n",
      "   +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME#10], functions=[partial_count(1)])\n",
      "      +- *(1) FileScan csv [DEST_COUNTRY_NAME#10] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/home/cloudera/spark-definitive/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>\n"
     ]
    }
   ],
   "source": [
    "sqlWay.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s pull out some interesting statistics from our data. One thing to understand is that DataFrames\n",
    "(and SQL) in Spark already have a huge number of manipulations available. There are hundreds of\n",
    "functions that you can use and import to help you resolve your big data problems faster. We will use\n",
    "the max function, to establish the maximum number of flights to and from any given location. This just\n",
    "scans each value in the relevant column in the DataFrame and checks whether it’s greater than the\n",
    "previous values that have been seen. This is a transformation, because we are effectively filtering\n",
    "down to one row. Let’s see what that looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(max(count)=370002)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"SELECT max(count) FROM flight_data_table\").take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s perform something a bit more\n",
    "complicated and find the top five destination countries in the data. This is our first multitransformation\n",
    "query, so we’ll take it step by step. Let’s begin with a fairly straightforward SQL\n",
    "aggregation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxsql = spark.sql(\"\"\"\n",
    "SELECT DEST_COUNTRY_NAME, sum(count) as destination_total\n",
    "FROM flight_data_table\n",
    "GROUP BY DEST_COUNTRY_NAME\n",
    "ORDER BY sum(count) DESC\n",
    "LIMIT 5\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|destination_total|\n",
      "+-----------------+-----------------+\n",
      "|    United States|           411352|\n",
      "|           Canada|             8399|\n",
      "|           Mexico|             7140|\n",
      "|   United Kingdom|             2025|\n",
      "|            Japan|             1548|\n",
      "+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "maxsql.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s move to the DataFrame syntax that is semantically similar but slightly different in\n",
    "implementation and ordering. But, as we mentioned, the underlying plans for both of them are the\n",
    "same. Let’s run the queries and see their results as a sanity check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|destination_total|\n",
      "+-----------------+-----------------+\n",
      "|    United States|           411352|\n",
      "|           Canada|             8399|\n",
      "|           Mexico|             7140|\n",
      "|   United Kingdom|             2025|\n",
      "|            Japan|             1548|\n",
      "+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightdata.groupBy(\"DEST_COUNTRY_NAME\")\\\n",
    ".sum(\"count\").withColumnRenamed(\"sum(count)\", \"destination_total\")\\\n",
    ".sort(desc(\"destination_total\")).limit(5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above sequence of transformations has 7 transformation steps: read->groupBy->sum->withColumnRenamed->sort->limit->collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "TakeOrderedAndProject(limit=5, orderBy=[destination_total#109L DESC NULLS LAST], output=[DEST_COUNTRY_NAME#10,destination_total#109L])\n",
      "+- *(2) HashAggregate(keys=[DEST_COUNTRY_NAME#10], functions=[sum(cast(count#12 as bigint))])\n",
      "   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#10, 5)\n",
      "      +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME#10], functions=[partial_sum(cast(count#12 as bigint))])\n",
      "         +- *(1) FileScan csv [DEST_COUNTRY_NAME#10,count#12] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/home/cloudera/spark-definitive/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,count:int>\n"
     ]
    }
   ],
   "source": [
    "flightdata.groupBy(\"DEST_COUNTRY_NAME\")\\\n",
    ".sum(\"count\").withColumnRenamed(\"sum(count)\", \"destination_total\")\\\n",
    ".sort(desc(\"destination_total\")).limit(5).explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
