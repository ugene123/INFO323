{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center\"> INFO 323: Cloud Computing and Big Data</h1>\n",
    "<h2 style=\"text-align:center\"> College of Computing and Informatics</h2>\n",
    "<h2 style=\"text-align:center\">Drexel University</h2>\n",
    "\n",
    "<h3 style=\"text-align:center\"> Introduction to Spark Programming (ch 2)</h3>\n",
    "<h3 style=\"text-align:center\"> Yuan An, PhD</h3>\n",
    "<h3 style=\"text-align:center\">Associate Professor</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD\n",
    "Data objects in Spark are immutable.\n",
    "\n",
    "A Resilient\tDistributed\tDataset\t(RDD) is the most fundamental data object used in Spark programming.\t\n",
    "\n",
    "RDDs are datasets within a Spark application, including the\tinitial\tdataset(s)\tloaded,\tany\tintermediate\tdataset(s),\tand\tthe\tfinal\tresultant dataset(s).\t\n",
    "\n",
    "Most Spark applications load an\tRDD\twith external data and\tthen create\tnew\tRDDs by\tperforming\toperations on the\texisting RDDs; these operations\tare\t**transformations**.\t\n",
    "\n",
    "This process is\trepeated until an output operation is ultimately required—for instance,\tto\twrite the results of\tan application to a filesystem;\tthese types\tof operations are actions.\n",
    "![Programming in Spark](info323-programming-in-spark.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Narrow Transformation\n",
    "Transformations consisting of narrow dependencies (we’ll call them narrow transformations) are\n",
    "those for which each input partition will contribute to only one output partition.\n",
    "## Wide Transformation\n",
    "A wide dependency (or wide transformation) style transformation will have input partitions\n",
    "contributing to many output partitions. You will often hear this referred to as a shuffle whereby Spark\n",
    "will exchange partitions across the cluster. With narrow transformations, Spark will automatically\n",
    "perform an operation called pipelining, meaning that if we specify multiple filters on DataFrames,\n",
    "they’ll all be performed in-memory. The same cannot be said for shuffles. When we perform a shuffle,\n",
    "Spark writes the results to disk. \n",
    "![narrow vs wide](info323-narrow-vs-wide.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lazy Evaluation\n",
    "Lazy evaluation means that Spark will wait until the very last moment to execute the graph of\n",
    "computation instructions. \n",
    "\n",
    "In Spark, instead of modifying the data immediately when you express some\n",
    "operation, you build up a plan of transformations that you would like to apply to your source data. \n",
    "\n",
    "By waiting until the last minute to execute the code, Spark compiles this plan from your raw DataFrame\n",
    "transformations to a streamlined physical plan that will run as efficiently as possible across the\n",
    "cluster. \n",
    "\n",
    "This provides immense benefits because Spark can optimize the entire data flow from end to\n",
    "end. \n",
    "\n",
    "An example of this is something called predicate pushdown on DataFrames. If we build a large\n",
    "Spark job but specify a filter at the end that only requires us to fetch one row from our source data,\n",
    "the most efficient way to execute this is to access the single record that we need. Spark will actually\n",
    "optimize this for us by pushing the filter down automatically.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing that you might use RDDs for is to parallelize raw data that you have stored in memory on\n",
    "the driver machine. For instance, let’s parallelize some simple numbers and create a DataFrame after\n",
    "we do so. We then can convert that to a DataFrame to use it with other DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "mydf = spark.sparkContext.parallelize([Row(1), Row(2), Row(3)]).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_1=1), Row(_1=2), Row(_1=3)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydf.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create RDD from Files\n",
    "Spark provides API methods to create RDDS from a file, files or the contents of a directory.\n",
    "\n",
    "Files can be of various formats, from unstructured text files, to semi-structured files such as JSON files, to structured data courses such as CSV files.\n",
    "\n",
    "```\n",
    "sc.textFile(name,\tminPartitions=None,\tuse_unicode=True)\n",
    "```\n",
    "\n",
    "The\ttextFile() method is\tused\tto\tcreate\tRDDs\tfrom\tfiles\t(compressed\tor\n",
    "uncompressed),\tdirectories,\tor\tglob\tpatterns\t(file\tpatterns\twith\twildcards).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_rdd = sc.textFile(\"words-short.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "words-short.txt MapPartitionsRDD[5] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['  For nimble thought can jump both sea and land,']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_rdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RDD' object has no attribute 'schema'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-4a302f639c64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwords_rdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'RDD' object has no attribute 'schema'"
     ]
    }
   ],
   "source": [
    "words_rdd.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Methods for Creating RDD\n",
    "\n",
    "**wholeTextFiles()**\n",
    "```\n",
    "sc.wholeTextFiles(path,\tminPartitions=None,\tuse_unicode=True)\n",
    "```\n",
    "The\twholeTextFiles() method\tlets you read a\tdirectory containing multiple files.\n",
    "\n",
    "**read.jdbc()**\n",
    "```\n",
    "spark.read.jdbc(url,\ttable,\n",
    "\t\t\t\tcolumn=None,\n",
    "\t\t\t\tlowerBound=None,\n",
    "\t\t\t\tupperBound=None,\n",
    "\t\t\t\tnumPartitions=None,\n",
    "\t\t\t\tpredicates=None,\n",
    "\t\t\t\tproperties=None)\n",
    "```\n",
    "\n",
    "**read.json()**\n",
    "```\n",
    "spark.read.json(path,\tschema=None)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating RDD Programmatically\n",
    "It\tis\tpossible\tto\tcreate\tan\tRDD\tprogrammatically\tfrom\tdata\tin\tyour\tprogram,\n",
    "whether\tthe\tdata\tis\tin\tlists,\tarrays,\tor\tcollections.\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "parall_rdd = sc.parallelize([0, 1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parall_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parall_rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_rdd = sc.range(0, 1000, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range_rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range_rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation\n",
    "Transformations\tare\toperations\tperformed\tagainst\tRDDs\tthat\tresult\tin\tthe creation\tof\tnew\tRDDs.\tCommon\ttransformations\tinclude\tmap\tand\tfilter functions.\tThe\tfollowing\texample\tshows\ta\tnew\tRDD\tcreated\tfrom a transformation\tof\tan\texisting\tRDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "even_num_rdd = parall_rdd.filter(lambda x: x % 2 ==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 4]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "even_num_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['For', 'nimble', 'thought', 'can', 'jump', 'both', 'sea', 'and', 'land,'],\n",
       " ['As', 'soon', 'as', 'think', 'the', 'place', 'where', 'he', 'would', 'be.'],\n",
       " ['But', 'ah,', 'thought', 'kills', 'me', 'that', 'I', 'am', 'not', 'thought'],\n",
       " ['To',\n",
       "  'leap',\n",
       "  'large',\n",
       "  'lengths',\n",
       "  'of',\n",
       "  'miles',\n",
       "  'when',\n",
       "  'thou',\n",
       "  'art',\n",
       "  'gone,'],\n",
       " ['But', 'that', 'so', 'much', 'of', 'earth', 'and', 'water', 'wrought,'],\n",
       " ['I', 'must', 'attend,', \"time's\", 'leisure', 'with', 'my', 'moan.'],\n",
       " ['Receiving', 'nought', 'by', 'elements', 'so', 'slow,'],\n",
       " ['But', 'heavy', 'tears,', 'badges', 'of', \"either's\", 'woe.'],\n",
       " [],\n",
       " []]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Map\n",
    "words_rdd.map(lambda x: x.split()).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['For',\n",
       " 'nimble',\n",
       " 'thought',\n",
       " 'can',\n",
       " 'jump',\n",
       " 'both',\n",
       " 'sea',\n",
       " 'and',\n",
       " 'land,',\n",
       " 'As']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flatmap\n",
    "words_rdd.flatMap(lambda x: x.split()).collect()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['As',\n",
       " 'as',\n",
       " 'he',\n",
       " 'me',\n",
       " 'I',\n",
       " 'am',\n",
       " 'To',\n",
       " 'of',\n",
       " 'so',\n",
       " 'of',\n",
       " 'I',\n",
       " 'my',\n",
       " 'by',\n",
       " 'so',\n",
       " 'of']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter\n",
    "words_rdd.flatMap(lambda x: x.split()).filter(lambda x: len(x) < 3).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_rdd.flatMap(lambda x: x.split()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# have to use flatMap followed by distinct. map() generates lists which cause errors when distinct()\n",
    "words_rdd.flatMap(lambda x: x.split()).distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('f', <pyspark.resultiterable.ResultIterable at 0x7fa750a82e48>),\n",
       " ('n', <pyspark.resultiterable.ResultIterable at 0x7fa750a902b0>),\n",
       " ('t', <pyspark.resultiterable.ResultIterable at 0x7fa750a906d8>),\n",
       " ('c', <pyspark.resultiterable.ResultIterable at 0x7fa750a90198>),\n",
       " ('j', <pyspark.resultiterable.ResultIterable at 0x7fa7507860f0>),\n",
       " ('b', <pyspark.resultiterable.ResultIterable at 0x7fa7507862b0>),\n",
       " ('s', <pyspark.resultiterable.ResultIterable at 0x7fa750786668>),\n",
       " ('a', <pyspark.resultiterable.ResultIterable at 0x7fa750a909b0>),\n",
       " ('l', <pyspark.resultiterable.ResultIterable at 0x7fa7507869b0>),\n",
       " ('p', <pyspark.resultiterable.ResultIterable at 0x7fa750786b00>),\n",
       " ('w', <pyspark.resultiterable.ResultIterable at 0x7fa750786b70>),\n",
       " ('h', <pyspark.resultiterable.ResultIterable at 0x7fa750786cf8>),\n",
       " ('k', <pyspark.resultiterable.ResultIterable at 0x7fa750786da0>),\n",
       " ('m', <pyspark.resultiterable.ResultIterable at 0x7fa750786e10>),\n",
       " ('i', <pyspark.resultiterable.ResultIterable at 0x7fa750786f98>),\n",
       " ('o', <pyspark.resultiterable.ResultIterable at 0x7fa750786978>),\n",
       " ('g', <pyspark.resultiterable.ResultIterable at 0x7fa75078c0f0>),\n",
       " ('e', <pyspark.resultiterable.ResultIterable at 0x7fa75078c160>),\n",
       " ('r', <pyspark.resultiterable.ResultIterable at 0x7fa75078c1d0>)]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_rdd.flatMap(lambda x: x.split()).groupBy(lambda x: x[0].lower()).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and', 'As', 'as', 'ah,', 'am']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_rdd.flatMap(lambda x: x.split()).sortBy(lambda x: x[0].lower()).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actions\n",
    "Actions\tin\tSpark either return\tvalues,\tas is the case with\tcount(); return\tdata, as is the\tcase with collect();\tor save\tdata externally,\tas\tis\tthe\n",
    "case\twith\tsaveAsTextFile().\tIn\tall\tcases,\tactions\tforce\tcomputation\tof\tan\n",
    "RDD\tand\tall\tof\tits\tparents.\tSome\tactions\treturn\teither\ta\tcount,\tan\taggregation\tof\n",
    "the\tdata,\tor\tpart\tor\tall\tof\tthe\tdata\tin\tan\tRDD.\tIn\tcontrast,\tforeach()\tis\tan\n",
    "action\tthat\tperforms\ta\tfunction\ton\teach\telement\tof\tan\tRDD.\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reduce\n",
    "parall_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parall_rdd.reduce(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordCount in Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sc.textFile(\"hdfs://quickstart.cloudera/user/cloudera/shakespeare.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is the 100th Etext file presented by Project Gutenberg, and']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124456"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = lines.flatMap(lambda x: x.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuples = words.map(lambda word: (word, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reduceByKey() method calls the lambda expression for all the tuples with the same word. The lambda expression has two arguments, a and b, which are the count values in two tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = tuples.reduceByKey(lambda x, y: (x+y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('This', 1105), ('is', 7851), ('the', 23242), ('100th', 1), ('Etext', 4)]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coalesce() method combines all the RDD partitions into a single partition since we want a single output file, and saveAsTextFile() writes the RDD to the specified location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts.coalesce(1).saveAsTextFile(\"hdfs://quickstart.cloudera/user/cloudera/wordcount/outputDir\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame\n",
    "DataFrame consists of a series of records (like rows in a table), that are of type Row,\n",
    "and a number of columns (like columns in a spreadsheet) that represent a computation expression that\n",
    "can be performed on each individual record in the Dataset. \n",
    "\n",
    "Schemas define the name as well as the\n",
    "type of data in each column. \n",
    "\n",
    "Partitioning of the DataFrame defines the layout of the DataFrame or\n",
    "Dataset’s physical distribution across the cluster. \n",
    "\n",
    "The partitioning scheme defines how that is\n",
    "allocated. You can set this to be based on values in a certain column or nondeterministically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_df = spark.range(500).toDF(\"number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(number=0), Row(number=1)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range_df.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df = spark.read.format(\"json\").load(\"2015-summary.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(DEST_COUNTRY_NAME,StringType,true),StructField(ORIGIN_COUNTRY_NAME,StringType,true),StructField(count,LongType,true)))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summary_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summary_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'count'>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_df[\"count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_df.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summary_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|          Singapore|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "summary_df.filter(col(\"count\") < 2).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**select and selectExpr**\n",
    "\n",
    "select and selectExpr allow you to do the DataFrame equivalent of SQL queries on a table of\n",
    "data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|DEST_COUNTRY_NAME|\n",
      "+-----------------+\n",
      "|    United States|\n",
      "|    United States|\n",
      "+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summary_df.select(\"DEST_COUNTRY_NAME\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+-------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "|    United States|            Romania|   15|        false|\n",
      "|    United States|            Croatia|    1|        false|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summary_df.selectExpr(\"*\", \"(DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withinCountry\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+---------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|numberOne|\n",
      "+-----------------+-------------------+-----+---------+\n",
      "|    United States|            Romania|   15|        1|\n",
      "|    United States|            Croatia|    1|        1|\n",
      "+-----------------+-------------------+-----+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "summary_df.withColumn(\"numberOne\", lit(1)).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DEST_COUNTRY_NAME', 'ORIGIN_COUNTRY_NAME', 'count', 'NumberOne']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_df.withColumn(\"numberOne\", lit(1)).withColumnRenamed(\"numberOne\", \"NumberOne\").columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|          Singapore|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summary_df.where(\"count < 2\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 5\n",
    "withReplacement = False\n",
    "fraction = 0.5\n",
    "summary_df.sample(withReplacement, fraction, seed).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataFrames = summary_df.randomSplit([0.25, 0.75], seed)\n",
    "dataFrames[0].count() > dataFrames[1].count() # False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|               Malta|      United States|    1|\n",
      "|Saint Vincent and...|      United States|    1|\n",
      "|       United States|            Croatia|    1|\n",
      "|       United States|          Gibraltar|    1|\n",
      "|       United States|          Singapore|    1|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summary_df.sort(\"count\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|     Burkina Faso|      United States|    1|\n",
      "|    Cote d'Ivoire|      United States|    1|\n",
      "|           Cyprus|      United States|    1|\n",
      "|         Djibouti|      United States|    1|\n",
      "|        Indonesia|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summary_df.orderBy(\"count\", \"DEST_COUNTRY_NAME\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|     Burkina Faso|      United States|    1|\n",
      "|    Cote d'Ivoire|      United States|    1|\n",
      "|           Cyprus|      United States|    1|\n",
      "|         Djibouti|      United States|    1|\n",
      "|        Indonesia|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summary_df.orderBy(col(\"count\"), col(\"DEST_COUNTRY_NAME\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
